{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART3:\n",
    "\n",
    "Step1.Load and Store data\n",
    "\n",
    "1.We stored the input data in List[List[Str]]\n",
    "\n",
    "2.1st List includ all sentences in a file\n",
    "\n",
    "3.2nd List includ all words in one sentence\n",
    "\n",
    "4.Str is one of the world in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "FOLDER = Path('./MachineLearning-Project/')\n",
    "\n",
    "\n",
    "#EN\n",
    "EN = FOLDER/'EN(2)'\n",
    "Train_EN = 'ENtrain'\n",
    "dev_x_EN = 'ENdev.in'\n",
    "dev_y_EN = 'ENdev.out'\n",
    "ENpart2_out = './EN(2)/dev.p2.out'\n",
    "ENpart3_out = './EN(2)/dev.p3.out'\n",
    "\n",
    "#CN\n",
    "CN = FOLDER/'CN'\n",
    "Train_CN = 'CNtrain'\n",
    "dev_x_CN = 'CNdev.in'\n",
    "dev_y_CN = 'CNdev.out'\n",
    "CNpart2_out = './CN/dev.p2.out'\n",
    "CNpart3_out = './CN/dev.p3.out'\n",
    "\n",
    "#SG\n",
    "SG = FOLDER/'SG(1)'\n",
    "Train_SG = 'SGtrain'\n",
    "dev_x_SG = 'SGdevc.in'\n",
    "dev_y_SG = 'SGdev.out'\n",
    "SGpart2_out = './SG(1)/dev.p2.out'\n",
    "SGpart3_out = './SG(1)/dev.p5.out'\n",
    "EVALscript ='evalResult.py'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2.\n",
    "\n",
    "Run HMM model and got Results using HMM model that we created\n",
    "Here is the HMM Algorithem that we defined\n",
    "\n",
    "\n",
    "1.At the beginning and end of sentence,we add <SOS> and <EOS> to show start and End.\n",
    "    \n",
    "2.We set rows are initial state\n",
    "\n",
    "3.We set columns are the state trasitted to.(next state)\n",
    "\n",
    "    \n",
    "4.We iterate each sentence for each file.\n",
    "    \n",
    "5.There is a $(|T|+1)\\times(|T|+1)$ matrix to store count value. \n",
    "    \n",
    "6.We calculate the probabilities by dividing each entry with the sum of correspodning row.\n",
    "\n",
    "7.Write an Viterbi Algorithm, implement two matrix.\n",
    "    \n",
    "    parent matrix(size $n\\times(|T|+1)$)\n",
    "    score matrix (size $n\\times(|T|+1)$)\n",
    "    \n",
    "8.set optimal path, which is restored by finding the argmax among $|T|$ tags in previous position\n",
    "\n",
    "\n",
    "9.Set k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary library\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set index\n",
    "def get_index_number(max_arg, index_value, kvalue):\n",
    "    count = 0\n",
    "    for o in max_arg:\n",
    "        if o == index_value:\n",
    "            return count\n",
    "        if o//kvalue == index_value//kvalue:\n",
    "            count += 1\n",
    "\n",
    "\n",
    "#Get data from training file \n",
    "def get_data(nameof_trainingfile):\n",
    "    with open(nameof_trainingfile) as f:\n",
    "        lineslis = f.readlines()\n",
    "    temx, temy = [], []\n",
    "    x,y = [],[]\n",
    "    \n",
    "    for sen in lineslis:\n",
    "        if len(sen) == 1:\n",
    "            assert(len(temx) == len(temy))\n",
    "            x.append(temx)\n",
    "            y.append(temy)\n",
    "            temx, temy = [], []\n",
    "            continue\n",
    "            \n",
    "        xv,yv = sen.split()\n",
    "        temx.append(xv)\n",
    "        temy.append(yv)\n",
    "    if len(temx) !=0:\n",
    "        x.append(temx)\n",
    "        y.append(temy)\n",
    "    assert(len(x)==len(y))\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "#Testing command \n",
    "#get_data(SG_train) \n",
    "#get_data()return two list, one is include all words for each sentence,\n",
    "#the other(y) is include all tags for each sentence\n",
    "#words,labels = get_data(SG_train)\n",
    "#print(words)\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Test data\n",
    "#x is the nested list of string\n",
    "#x_int is the nested list of integer\n",
    "def get_tdata(file, w2index):\n",
    "    with open(file) as f:\n",
    "        lineslis =f.readlines()\n",
    "    x =[]\n",
    "    temx =[]\n",
    "    for sen in lineslis:\n",
    "        if len(sen.strip())== 0:\n",
    "            x.append(temx)\n",
    "            temx  = []\n",
    "            continue\n",
    "        xv = sen.split()\n",
    "        temx.append(xv[0])\n",
    "        \n",
    "    if len(temx) != 0:\n",
    "        x.append(temx)\n",
    "    x_int = [[w2index[oo] for oo in o] for o in x ]\n",
    "    return x, x_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a HMM class\n",
    "#Find emission parameters & Transition parameters\n",
    "#Run viteribi algorithem ---- Encode & Decode \n",
    "\n",
    "class HMM:\n",
    "    def __init__(self, tf):\n",
    "        # read data\n",
    "        # create vocab\n",
    "        # Set tagslabel at satrt state and final state SOS &EOS\n",
    "        \n",
    "        self.words, self.labels = get_data(tf)\n",
    "        self.labeltags = list(set([oo for o in self.labels for oo in o])) + ['SOS', 'EOS']\n",
    "        self.tag2index = {o:i for i,o in enumerate(self.labeltags)}\n",
    "        vocab_count = Counter([oo for o in self.words for oo in o])\n",
    "        self.vocab = [o for o, v in dict(vocab_count).items() if v>=3] + ['#UNK#']\n",
    "        self.word2index = defaultdict(int)\n",
    "        \n",
    "        for i,o in enumerate(self.vocab): self.word2index[o] = i+1\n",
    "        self.x = [[self.word2index[oo] for oo in o] for o in self.words]\n",
    "        self.y = [[self.tag2index[oo] for oo in o] for o in self.labels]\n",
    "    \n",
    "    def train(self):\n",
    "        self.transition_para()\n",
    "        self.emission_para()\n",
    "    \n",
    "    def transition_para(self):\n",
    "        \"\"\"\n",
    "        Step : \n",
    "        1. Rows: transition from current state : (v1, v2, ..., 'SOS')\n",
    "        2. Cols: to next state :   (v1, v2, ..., 'EOS')\n",
    "        3. two new state sos , eos \n",
    "        4. START transition\n",
    "           Set empty array, add value in \n",
    "        5. label tags transition from position 0 to len(yvalue)-2\n",
    "        6. STOP transition\n",
    "        \n",
    "        \"\"\"\n",
    "        transition = np.zeros((len(self.labeltags)-1, len(self.labeltags)-1))\n",
    "        \n",
    "        for yvalue in self.y: \n",
    "            transition[-1, yvalue[0]] += 1 \n",
    "            for i in range(len(yvalue)-1): \n",
    "                transition[yvalue[i], yvalue[i+1]] += 1\n",
    "            transition[yvalue[-1], -1] += 1 \n",
    "        transition = transition/np.sum(transition, axis=1)\n",
    "        self.transition = transition\n",
    "    \n",
    "    \n",
    "    def emission_para(self):\n",
    "        \n",
    "        emission_matrix = np.zeros((len(self.vocab), len(self.labeltags)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        fx = [oo for o in self.x for oo in o]\n",
    "        fy = [oo for o in self.y for oo in o]\n",
    "        \n",
    "        \n",
    "        for xvalue, yvalue in zip(fx,fy):\n",
    "            emission_matrix[xvalue, yvalue] += 1\n",
    "    \n",
    "        yc = np.zeros(len(self.labeltags))\n",
    "        \n",
    "        for yvalue in fy:\n",
    "            yc[yvalue] += 1\n",
    "        \n",
    "        emission_matrix  = emission_matrix / yc[None, :]\n",
    "        np.nan_to_num(emission_matrix , 0)\n",
    "        self.emission = emission_matrix \n",
    "    \n",
    "    def predict_top_k(self, inpfile, outfile, k=1):\n",
    "        \n",
    "        assert hasattr(self, 'transition') and hasattr(self, 'emission'), \"run self.train() first\"\n",
    "        with open(outfile, 'w') as f:\n",
    "            words, dev_x = get_tdata(inpfile, self.word2index)\n",
    "            slis = []\n",
    "            for i, (wsvalue,o) in enumerate(zip(words, dev_x)):\n",
    "                path, logmax= self.viterbi(o, k)\n",
    "                slis.append(logmax)\n",
    "                for w, p in zip(wsvalue, path):\n",
    "                    f.write(w + ' ' + self.labeltags[p] + '\\n')\n",
    "                f.write('\\n')\n",
    "        return\n",
    "    \n",
    "    def viterbi(self, x, k=1):\n",
    "        score = np.zeros( (len(x)+2, len(self.labeltags)-2, k) ) \n",
    "        argmax = np.zeros( (len(x)+2, len(self.labeltags)-2, k), dtype=np.int) \n",
    "        transition, emission = np.log(self.transition), np.log(self.emission)\n",
    "        # initialization at j=1\n",
    "        score[1, :, 0] = (transition[-1, :-1] + emission[x[0], :-2])\n",
    "        score[1, :, 1:] = -np.inf\n",
    "        # j=2, ..., n\n",
    "        for j in range(2, len(x)+1): \n",
    "            for t in range(len(self.labeltags)-2):\n",
    "                pi = score[j-1, :]  # (num_of_labeltags-2, 7)\n",
    "                a = transition[:-1, t] # (num_of_labeltags-2,)\n",
    "                b = emission[x[j-1], t] # (1,)\n",
    "                previous_all_scores = ((pi + a[:, None])).flatten()\n",
    "                topk = previous_all_scores.argsort()[-k:][::-1] # big to small\n",
    "                argmax[j, t] = topk # big: 0, small: -1 # topk // k is the real argument\n",
    "                score[j, t] = previous_all_scores[topk] + b\n",
    "\n",
    "        # j=n+1 step\n",
    "        pi = score[len(x)] # (num_of_labeltags-2, 7)\n",
    "        a = transition[:-1, -1]\n",
    "        argmax_stop = (pi + a[:,None]).flatten().argsort()[-k:][::-1] # big to small\n",
    "        log_likelihood = np.min(pi+a[:,None])\n",
    "        argmax = argmax[2:-1] # (len(x)-1, num_of_labeltags-2, 7)\n",
    "        # decoding\n",
    "\n",
    "        ## Initialize the last pointer backward\n",
    "        temp_index = argmax_stop[-1] # range from 0 ~ k * ( len(labeltags) - 1 )\n",
    "        temp_index_index = get_index_number(argmax_stop, temp_index, k) # range from 0 ~ k-1, next index in next tag\n",
    "        temp_arg = temp_index // k # range from 0 ~ k-1, next tag index\n",
    "        \n",
    "        path = [argmax_stop[-1]//k] # initialize path as an array\n",
    "\n",
    "        for i in range(len(argmax)-1, -1, -1):\n",
    "            temp_index = argmax[i, temp_arg, temp_index_index]\n",
    "            temp_index_index = get_index_number(argmax[i, temp_arg], temp_index, k)\n",
    "            temp_arg = temp_index // k\n",
    "            path.append(temp_arg)\n",
    "        return path[::-1], log_likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part3 Result\n",
    "\n",
    "1.Run HMM Model\n",
    "\n",
    "2.Run train() method \n",
    "\n",
    "3.Run Eval_Script to get the precision, recall and F score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM MODEL -----Test SG\n",
    "hmm = HMM(Train_SG)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(dev_x_SG, SGpart3_out, k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 4301\r\n",
      "#Entity in prediction: 3125\r\n",
      "\r\n",
      "#Correct Entity : 1901\r\n",
      "Entity  precision: 0.6083\r\n",
      "Entity  recall: 0.4420\r\n",
      "Entity  F: 0.5120\r\n",
      "\r\n",
      "#Correct Sentiment : 1686\r\n",
      "Sentiment  precision: 0.5395\r\n",
      "Sentiment  recall: 0.3920\r\n",
      "Sentiment  F: 0.4541\r\n"
     ]
    }
   ],
   "source": [
    "!python3 evalResult.py SGdev.out SGdev.p3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM MODEL -----Test EN\n",
    "hmm = HMM(Train_EN)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(dev_x_EN, ENpart3_out, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 13179\r\n",
      "#Entity in prediction: 12733\r\n",
      "\r\n",
      "#Correct Entity : 10800\r\n",
      "Entity  precision: 0.8482\r\n",
      "Entity  recall: 0.8195\r\n",
      "Entity  F: 0.8336\r\n",
      "\r\n",
      "#Correct Sentiment : 10387\r\n",
      "Sentiment  precision: 0.8158\r\n",
      "Sentiment  recall: 0.7881\r\n",
      "Sentiment  F: 0.8017\r\n"
     ]
    }
   ],
   "source": [
    "!python3 evalResult.py Endev.out ENdev.p3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HMM MODEL -----Test CN\n",
    "hmm = HMM(Train_CN)\n",
    "hmm.train()\n",
    "hmm.predict_top_k(dev_x_CN, CNpart3_out, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "#Entity in gold data: 700\r\n",
      "#Entity in prediction: 311\r\n",
      "\r\n",
      "#Correct Entity : 106\r\n",
      "Entity  precision: 0.3408\r\n",
      "Entity  recall: 0.1514\r\n",
      "Entity  F: 0.2097\r\n",
      "\r\n",
      "#Correct Sentiment : 71\r\n",
      "Sentiment  precision: 0.2283\r\n",
      "Sentiment  recall: 0.1014\r\n",
      "Sentiment  F: 0.1405\r\n"
     ]
    }
   ],
   "source": [
    "!python3 evalResult.py CNdev.out CNdev.p3.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
